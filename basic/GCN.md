# 1. 特征工程

**特征工程**：从原始数据利用领域知识抽取特征的过程。[-wiki](https://en.wikipedia.org/wiki/Feature_engineering)

## 1.1. 特征选择

**特征选择**：目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。

假设有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据（irrelevant data），我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等

**主要方法**：

- 过滤(filter)：主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。
- 封装(wrapper):实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。
- 嵌入(embeded)：学习器自身自主选择特征，如使用Regularization做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用Random Forest和Gradient boosting做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。

## 1.2. 特征提取

**特征提取**:自动地构建新的特征，将原始特征转换为一组具有明显物理意义（Gabor、几何特征[角点、不变量]、纹理[LBP HOG]）或者统计意义或核的特征。

对于图像数据，可能还包括了线或边缘检测。

## 1.3. 特征构建

**特征构建**：是从原始数据中人工的构建新的特征。

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用混合属性或者组合属性来创建新的特征，或是分解或切分原有的特征来创建新的特征。

# 图嵌入(Graph Embedding)

## 嵌入

**简单来说**，嵌入是将复杂的东西（比如视频、文本）映射到一个更低的固定维度的向量，这些向量capture关键特征。嵌入通常以**无监督**、**可泛化**的方式学习什么是重要的

密集表示的好处是泛化能力好

### 词嵌入(Word Embedding)

**原始方法**
手工设置规则，比如猫、狗都是动物

**基于计数的方法**
用一个词频矩阵记录每个词在document中出现的次数
缺点：
1. 需要很多的document理解word在上下文的含义。并且矩阵会越来越大
2. 巨大的稀疏矩阵或向里是麻烦且无意义的

