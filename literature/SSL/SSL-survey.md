# A survey on semi-supervised learning-2020

[A survey on semi-supervised learning-2020](https://link.springer.com/article/10.1007/s10994-019-05855-6)

> 本文涵盖了早起工作以及最新的进展（2020）。
> 本文重点是最突出且最相关的工作。
> 提出了一种新的半监督分类算法的分类方法，并且阐明了将未标记数据纳入训练过程的不同概念和方法。
> 说明了大多数半监督学习算法所基于的基本假设是如何相互紧密联系的，以及如何与半监督聚类假设相关联。

## 半监督学习的假设

半监督学习的一个必要条件是：输入空间上的边缘分布$p(x)$包含后验分布$p(y|x)$的信息。否则无法提高预测的准确性。

### 平滑性假设

两个输入数据点$x,x'\in X$在输入空间上相邻，相应的标签$y,y'$应该相同。  
在半监督学习中，该假设可以传递地应用与未标签数据。

### 低密度假设

分类器的决策边界最好通过输入空间中的低密度区域。本质上意味着决策边界应该位于数据点少的区域。决策边界穿过低密度区域（该区域没有任何一对相似数据点），不会违反平滑性假设。因此低密度假设与平滑性假设密切相关。

### 流形假设

1. 输入空间上的所有数据点由多个低纬流形组成。
2. 位于同一个流形上的数据点具有相同的标签。

## 聚类

### 聚类假设

**同一簇的数据点属于同一类**——[Chapelle et al.2006](http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf)  
输入空间$\mathcal{X}$以及数据集$X\subset \mathcal{X}$。一个簇是一个数据点集合$C\subseteq \mathcal{X}$，其相似度高于其他数据点。聚类相当于找一个函数$f:X\rightarrow \mathcal{Y}$，将每个数据点$x\in X$映射到一个聚类标签$y=f(x)$。  
相似性的概念唯一地定义了$p(x)和p(y|x)$的相互作用。因此可以从两个点彼此之间以及与其他点的相似性得出两个点是否属于同一聚类。
聚类假设对应于半监督学习的必要条件：$p(x)$包含有关$p(y|x)$的信息。

## 半监督学习什么时候起作用

我们不应该认为：仅仅通过引入无标签数据，半监督学习就能够提高预测性能。  
半监督学习是寻找和配置学习算法的另外一种方向。

## 半监督算法的实证评估

在比较和评估机器学习算法时，许多决策都会影响算法的相对性能。[Oliver.2018](https://arxiv.org/abs/1804.09170)为半监督学习算法的现实评估建立了一套指南。 （实验方法）  
